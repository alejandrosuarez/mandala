{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in pytorch\n",
    "## What's in this tutorial?\n",
    "This notebook will walk you through the basic uses of `mandala` for storing\n",
    "and tracking ML experiment results. It uses logistic regression on a synthetic\n",
    "dataset as a \"minimally interesting\" example of a data management use case. By\n",
    "following this ML mini-project, you will learn how to\n",
    "- break up an experiment into Python functions whose calls can be\n",
    "tracked and queried by `mandala`;\n",
    "- use `mandala`'s memoization to avoid re-running expensive computations and to\n",
    "naturally interact with and grow your project (by adjusting the parameters and/or\n",
    "adding new code);\n",
    "- repurpose the (pure Python) code of your experiments into a *query interface*\n",
    "to their results \"for free\";\n",
    "- change and create new versions of your experimental primitives and have them\n",
    "  seamlessly interact with the results of previous runs.\n",
    "\n",
    "Ultimatley, the features of `mandala` work together to enable you to evolve\n",
    "complex ML projects by writing only the plain-Python code that you'd write in a\n",
    "temporary in-memory interactive session, yet get the benefits of a\n",
    "database-backed experiment tracking system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.utils as utils\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.random.set_rng_state(torch.manual_seed(0).get_state())\n",
    "\n",
    "# recommended way to import mandala functionality\n",
    "from mandala_lite.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiment primitives\n",
    "For the purposes of this tutorial, you'll break the project into two main\n",
    "functions: to generate the dataset, and to train the model. Below is fairly\n",
    "standard `pytorch` code for these; note the use of `@op` to mark the functions\n",
    "as tracked by `mandala`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIMENSION = 10\n",
    "\n",
    "# `mandala` decorator; like @functools.lru_cache, but with extra functionality.\n",
    "# Currently, you must specify the exact number of inputs (i.e., no *args or **kwargs),\n",
    "# and the number of outputs (via a type annotation).\n",
    "@op\n",
    "def generate_dataset() -> Tuple[TensorDataset, TensorDataset]:\n",
    "    n_samples = 1000\n",
    "    x = np.random.randn(n_samples, DATA_DIMENSION)\n",
    "    y = x[:, 0] > 0\n",
    "    # convert to torch tensors\n",
    "    x, y = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    # create train/test split with 80/20 ratio\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    train_dataset = TensorDataset(x[:train_size], y[:train_size])\n",
    "    test_dataset = TensorDataset(x[train_size:], y[train_size:])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(DATA_DIMENSION, 2)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        output = self.linear(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "@op\n",
    "def train_model(\n",
    "    train_dataset: TensorDataset,\n",
    "    test_dataset: TensorDataset,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 100,\n",
    "    num_epochs: int = 5,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # train a logistic regression model with the given loaders and hyperparameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for batch_index, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss_value = loss(output, labels)\n",
    "            loss_value.backward()\n",
    "        optimizer.step()\n",
    "        # test\n",
    "        accurate, total = 0, 0\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accurate += (predicted == labels).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, round(float(acc.item()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline and log the results\n",
    "Now that you have defined the functions that make up your pipeline, you can\n",
    "run it with the default parameters and log the results. \n",
    "\n",
    "The `@op` decorator on the functions above tells `mandala` to track the calls to\n",
    "these functions, and to store the results - but this only happens when you call\n",
    "these functions *in the context of a given `Storage` object*. So go ahead, and\n",
    "create a storage for the project: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This storage will hold the results of all the experiments you run in this\n",
    "notebook. Now, run the pipeline by wrapping the code you'd normally write in a \n",
    "`storage.run()` context manager; `mandala` will automatically track the calls to\n",
    "the functions you marked with `@op` and store the results in the `storage`\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    model, acc = train_model(train_dataset, test_dataset)\n",
    "    print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "A lot happened behind the scenes in these few lines of code! Let's break it\n",
    "down:\n",
    "- Inside the `storage.run()` block, each time an `@op`-decorated function is\n",
    "called **for the first time** on a set of inputs, `mandala` stores the inputs\n",
    "and outputs of this call in the storage. \n",
    "- Values shared between calls are stored only once. So\n",
    "  `train_dataset` will appear in storage as both the output to the call to\n",
    "  `generate_dataset`, and the input to the call to `train_model`.\n",
    "- The `acc` object (like all objects returned by `@op`-decorated functions) is a\n",
    "*value reference*, which is a value wrapped with storage-related metadata. \n",
    "\n",
    "So, what happens when you call `@op`-decorated functions *a second time* on the\n",
    "same inputs? Find out by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    model, acc = train_model(train_dataset, test_dataset)\n",
    "    print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that this time the intermediate training results did not get printed!**.\n",
    "This is because `mandala` recognized that the inputs to the functions were the\n",
    "same as before, and so it didn't need to re-run the calls. This is also evident\n",
    "from the lack of output from the model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grow the project with new parameters\n",
    "Running the pipeline once is nice, but where `mandala` really shines is in\n",
    "enabling you to grow a computational project in various ways with the minimal\n",
    "necessary code changes, and have the storage interfaces \"just work\". \n",
    "\n",
    "Let's begin exploring this by investigating the effect of changing the learning\n",
    "rate of the model. So far, you have been using the default learning rate of\n",
    "`0.001`. Let's try a few other values, but also see how they compare with the\n",
    "default value. Thanks to memoization, this is easy to do without re-doing\n",
    "expensive work: we can use a list of values for the `learning_rate` parameter\n",
    "that includes the default, and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for learning_rate in [0.001, 0.01, 0.1]:\n",
    "        model, acc = train_model(train_dataset, test_dataset, learning_rate)\n",
    "        print(\n",
    "            f\"===end of run=== learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first run was re-used from before, while the 2nd and 3rd were\n",
    "freshly computed. We see that the higher the learning rate, the better the final\n",
    "accuracy.  Now, let's try varying the batch size as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the results\n",
    "By now, you have run the pipeline with many different combinations of\n",
    "parameters, and it's getting difficult to make sense of all the results so far.\n",
    "One option to get to the results is to just re-run the above workflow, or a\n",
    "\"sub-workflow\" of it. \n",
    "\n",
    "For example, how might you get all the results for a given learning rate, e.g.\n",
    "`learning_rate=0.1`? One answer: just by re-running the subset of the above code\n",
    "using this value of the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.1]:  # only change relative to previous cell\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of storage access pattern is called **retracing**: you \"retrace\"\n",
    "computational code that you have already run in order to recover the quantities\n",
    "computed along the way. You can use retracing to query existing results (like\n",
    "you did above), or to easily add new parameters/logic that need to compute over\n",
    "existing results.\n",
    "\n",
    "However, sometimes you don't have a specific piece of code to retrace and just\n",
    "want to look at all the results in storage. For this, you can use a \"full\n",
    "storage search\" query interface via the `storage.query()` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    learning_rate = Q().named(\"learning_rate\")\n",
    "    model, acc = train_model(train_dataset, test_dataset, learning_rate, batch_size)\n",
    "    df = q.get_table(batch_size, learning_rate, acc.named(\"accuracy\"))\n",
    "\n",
    "df.sort_values(by=[\"accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "In the `storage.query()` context manager, calls to `@op`-decorated functions\n",
    "build a computational graph of variables and constraints behind the scenes:\n",
    "- **variables** stand for a value stored in storage. These include the `Q()`\n",
    "objects above, as well as the values returned by `@op`-decorated functions.\n",
    "- **constraints** (i.e. function calls) enforce relationships between variables.\n",
    "\n",
    "The result of the query is a table, where each row is a choice of values for \n",
    "the variables that satisfy the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a larger dataset, or: how to modify memoized functions gracefully\n",
    "A very common scenario in ML is to extend an existing experimental primitive\n",
    "with new functionality, for example by exposing a hard-coded parameter, or\n",
    "adding a new behavior to the function (e.g., an option to use a different\n",
    "algorithm). `mandala` allows you to do this gracefully, seamlessly integrating \n",
    "the new functionality with the results of previous runs.\n",
    "\n",
    "For example, suppose you want to improve the accuracy numbers above. So far, you\n",
    "have been running all models on a synthetic dataset of size `1000`. Let's try\n",
    "increasing the dataset size while still sampling from the same distribution and\n",
    "see if that helps! \n",
    "\n",
    "To do this, simply modify the `generate_dataset` function to take an additional\n",
    "argument `n_samples` with a default value of `1000`. Let's also print out a\n",
    "message when it's called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def generate_dataset(n_samples: int = 1000) -> Tuple[TensorDataset, TensorDataset]:\n",
    "    print(\"Hi from `generate_dataset` with an extra argument!\")\n",
    "    x = np.random.randn(n_samples, DATA_DIMENSION)\n",
    "    y = x[:, 0] > 0\n",
    "    # convert to torch tensors\n",
    "    x, y = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    # create train/test split with 80/20 ratio\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    train_dataset = TensorDataset(x[:train_size], y[:train_size])\n",
    "    test_dataset = TensorDataset(x[train_size:], y[train_size:])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the memoized calls after we change the function? Find out by\n",
    "re-running the project so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate_dataset` was quiet, indicating that it was not\n",
    "re-run! This is because `mandala` recognized that the inputs to the function\n",
    "were the default values, and so it didn't need to re-run the call. Note that you\n",
    "must make the default values consistent with the previous runs of the function;\n",
    "alternatively, you can set them to `None` and deal with this logic inside the\n",
    "function. \n",
    "\n",
    "Now let's finally run the pipeline with a larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a nice table with the results; you can do this by minimally\n",
    "modifying the query code from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    n_samples = Q().named(\"n_samples\")\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples)\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    learning_rate = Q().named(\"learning_rate\")\n",
    "    model, acc = train_model(train_dataset, test_dataset, learning_rate, batch_size)\n",
    "    df = q.get_table(n_samples, batch_size, learning_rate, acc.named(\"accuracy\"))\n",
    "\n",
    "df.sort_values(by=[\"accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, it looks like the larger dataset didn't help. Why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Oh no, there's a bug!\", or: function versioning\n",
    "If you look at the `train_model` function carefully, you'll see there's a\n",
    "subtle but crucial mistake: because the call to `optimizer.step()` happens\n",
    "outside the loop over the batches, the model is only updated once per epoch!\n",
    "\n",
    "Unfortunately, we have already ran a lot of experiments with this bug. How can\n",
    "we fix this without messing up our storage and re-running more than we need to?\n",
    "Starting the whole project from scratch is not great, because it means we would\n",
    "have to regenerate the dataset too. \n",
    "\n",
    "Fortunately, `mandala` has a simple way to deal with this situation with the\n",
    "minimal possible amount of code changes. You must do two things:\n",
    "- change the function to fix the bug, and **increment the\n",
    "  version number**. By default, each function starts at version `0`. \n",
    "- re-run the experiments that used the old version of the function. All results\n",
    "  that do not depend on the bug will be reused, and only the results that\n",
    "  depended on the bug will be recomputed (if the bug fix happens to lead to\n",
    "  different values of the outputs).\n",
    "\n",
    "So, fix the bug and increment the version number in the `@op` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(version=1)\n",
    "def train_model(\n",
    "    train_dataset: TensorDataset,\n",
    "    test_dataset: TensorDataset,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 100,\n",
    "    num_epochs: int = 5,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # train a logistic regression model with the given loaders and hyperparameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        for batch_index, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss_value = loss(output, labels)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        # test\n",
    "        accurate, total = 0, 0\n",
    "        for images, labels in test_loader:  #! bugfix here\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accurate += (predicted == labels).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, round(float(acc.item()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, literally copy-paste the computation + query code from above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )\n",
    "\n",
    "with storage.query() as q:\n",
    "    n_samples = Q().named(\"n_samples\")\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples)\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    learning_rate = Q().named(\"learning_rate\")\n",
    "    model, acc = train_model(train_dataset, test_dataset, learning_rate, batch_size)\n",
    "    df = q.get_table(n_samples, batch_size, learning_rate, acc.named(\"accuracy\"))\n",
    "\n",
    "df.sort_values(by=[\"accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now we have the correct results. Maybe. At least, they make more sense than\n",
    "before: accuracy went up, and the larger dataset helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model mistakes, or: easily track workflows with complex logic\n",
    "So far, you have been running the same two-part pipeline independently for\n",
    "several sets of parameters. This kind of workflow is common in ML projects, and\n",
    "many libraries provide e.g. hyperparameter tuning tools for such cases (e.g., \n",
    "`sklearn`). \n",
    "\n",
    "However, real-world experiments often go beyond these \"embarrassingly DAG-like\"\n",
    "workflows, and require more complex logic. This is another place where `mandala`\n",
    "shines: thanks to the transparent way you build experiments directly in plain\n",
    "Python, you have full control over the logic of your experiments, while getting\n",
    "the benefits of a storage system that \"just works\". \n",
    "\n",
    "To illustrate this, let's compare the mistakes made by models trained with the\n",
    "same batch size and dataset, but different learning rates. First, define a\n",
    "function that computes the Jaccard index between the sets of mistakes of two\n",
    "models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def compare_mistakes(\n",
    "    model_1: LogisticRegression,\n",
    "    model_2: LogisticRegression,\n",
    "    test_dataset: TensorDataset,\n",
    ") -> float:\n",
    "    # compute the Jaccard index between the mistakes of the two models\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_1 = model_1.to(device)\n",
    "    model_2 = model_2.to(device)\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    mistakes_1 = set()\n",
    "    mistakes_2 = set()\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output_1 = model_1(images)\n",
    "        output_2 = model_2(images)\n",
    "        _, predicted_1 = torch.max(output_1.data, 1)\n",
    "        _, predicted_2 = torch.max(output_2.data, 1)\n",
    "        for i in range(len(labels)):\n",
    "            if predicted_1[i] != labels[i]:\n",
    "                mistakes_1.add(i)\n",
    "            if predicted_2[i] != labels[i]:\n",
    "                mistakes_2.add(i)\n",
    "    return round(\n",
    "        len(mistakes_1.intersection(mistakes_2)) / len(mistakes_1.union(mistakes_2)), 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the mistakes of all the models trained on the synthetic dataset of\n",
    "size `2000` for each batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                jaccard_index = compare_mistakes(model_1, model_2, test_dataset)\n",
    "                print(\n",
    "                    f\"===end of run=== batch_size: {batch_size}, lr_1: {lr_1}, lr_2: {lr_2}, jaccard_index: {unwrap(jaccard_index)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, note that thanks to the memoization, the only new computations here\n",
    "were calls to the new `compare_mistakes` function! Despite rearranging the code\n",
    "a bit instead of just copy-pasting it, the results are still reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the new workflow\n",
    "How might you get a nice table with the results of the above workflow? Again,\n",
    "you have two options. \n",
    "\n",
    "The simplest is just to retrace the code that you have already run, and collect \n",
    "the results as rows of a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    rows = []\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                jaccard = compare_mistakes(model_1, model_2, test_dataset)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"lr_1\": lr_1,\n",
    "                        \"lr_2\": lr_2,\n",
    "                        \"jaccard\": unwrap(jaccard),\n",
    "                    }\n",
    "                )\n",
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(by=[\"jaccard\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `storage.query()` context manager, but you need to be\n",
    "careful to specify the correct constraints! In particular, you must create two\n",
    "placeholders for the learning rate (why?). \n",
    "\n",
    "The easiest way to build a query is to copy-paste the computational code, and\n",
    "then modify it to replace iteration over a parameter range with a placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    n_samples = Q().named(\"n_samples\")\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    lr_1 = Q().named(\"lr_1\")\n",
    "    lr_2 = Q().named(\"lr_2\")\n",
    "    model_1, acc_1 = train_model(train_dataset, test_dataset, lr_1, batch_size)\n",
    "    model_2, acc_2 = train_model(train_dataset, test_dataset, lr_2, batch_size)\n",
    "    jaccard_index = compare_mistakes(model_1, model_2, test_dataset)\n",
    "    df = q.get_table(n_samples, batch_size, lr_1, lr_2, jaccard_index.named(\"jaccard\"))\n",
    "\n",
    "df.sort_values(by=[\"jaccard\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### The importance of a good function decomposition\n",
    "Decomposing a project like this one into functions (i.e., `generate_dataset` and\n",
    "`train_model`) is something you often find yourself doing almost without\n",
    "thinking. It's a necessary practice in software engineering and ML\n",
    "experimentation in general. \n",
    "\n",
    "However, it is also particularly important to find a \"good\" decomposition when\n",
    "tracking experiments with `mandala`, because the kind of decomposition you use\n",
    "has consequences on the performance of your storage system.\n",
    "\n",
    "For example, consider the alternative of having a single function that does both\n",
    "the dataset generation and the model training:\n",
    "```python\n",
    "def generate_and_train(n_samples, batch_size, learning_rate) -> Tuple[LogisticRegression, float]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "A frequent use case is to train many models on the same dataset, but with\n",
    "different hyperparameters. With a single function, you'd have to re-run the\n",
    "dataset generation function every time, even though the dataset is the same!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e39bb3b1f45b78879464f3858f3ac405da62799496d9b7e0a39caf0b676c9a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
