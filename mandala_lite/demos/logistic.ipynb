{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in pytoch with mandala\n",
    "This is a simple example of logistic regression in pytorch, using mandala for\n",
    "data management. We will use the MNIST dataset to train a logistic regression\n",
    "model and play with the hyperparameters of the training components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.utils as utils\n",
    "from mandala_lite.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28**2\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(INPUT_SIZE, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        output = self.linear(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "@op\n",
    "def get_dataloaders(\n",
    "    batch_size: int = 100, train_size: int = 1_000\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_data = Subset(\n",
    "        MNIST(\"data\", train=True, download=True, transform=tv.transforms.ToTensor()),\n",
    "        indices=range(train_size),\n",
    "    )\n",
    "    test_data = Subset(\n",
    "        MNIST(\"data\", train=False, transform=tv.transforms.ToTensor()),\n",
    "        indices=range(10_000),\n",
    "    )\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "@op\n",
    "def train_lr(\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    learning_rate: float = 0.001,\n",
    "    num_epochs: int = 5,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        for batch_index, (images, labels) in enumerate(train_loader):\n",
    "            images = images.view(-1, INPUT_SIZE).to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss_value = loss(output, labels)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        # test\n",
    "        accurate, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, INPUT_SIZE).to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accurate += (predicted == labels).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, float(acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a storage for the results\n",
    "Some explanation on the relationship between the storage and the `@op`-decorated\n",
    "functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "This is just a single run of the pipeline with default parameters to see a\n",
    "simple use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training loss: 2.27. Test accuracy: 7.09\n",
      "Epoch: 1, Training loss: 2.27. Test accuracy: 8.23\n",
      "Epoch: 2, Training loss: 2.28. Test accuracy: 9.56\n",
      "Epoch: 3, Training loss: 2.27. Test accuracy: 11.35\n",
      "Epoch: 4, Training loss: 2.22. Test accuracy: 13.62\n",
      "Final accuracy: 13.62\n"
     ]
    }
   ],
   "source": [
    "with storage.run():\n",
    "    train_loader, test_loader = get_dataloaders()\n",
    "    model, acc = train_lr(train_loader, test_loader)\n",
    "    print(f\"Final accuracy: {round(unwrap(acc), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's instructive to run the pipeline again to demonstrate the memoization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 13.62\n"
     ]
    }
   ],
   "source": [
    "with storage.run():\n",
    "    train_loader, test_loader = get_dataloaders()\n",
    "    model, acc = train_lr(train_loader, test_loader)\n",
    "    print(f\"Final accuracy: {round(unwrap(acc), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore parameters while reusing past results\n",
    "- Here, could have a few rounds of exploration, gradually expanding the search\n",
    "space, to show that it's easy to directly adjust the code and reuse results.\n",
    "- It would also be nice to later on alternate between this exploration and the\n",
    "queries, while introducing extra parameters (the batch and training set sizes),\n",
    "and evolving the run/query blocks alongside each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training loss: 2.29. Test accuracy: 11.06\n",
      "===End of run=== num_epochs: 1, learning_rate: 0.001, acc: 11.06\n",
      "Epoch: 0, Training loss: 2.33. Test accuracy: 9.15\n",
      "Epoch: 1, Training loss: 2.32. Test accuracy: 9.97\n",
      "===End of run=== num_epochs: 2, learning_rate: 0.001, acc: 9.97\n",
      "Epoch: 0, Training loss: 2.24. Test accuracy: 21.21\n",
      "Epoch: 1, Training loss: 2.22. Test accuracy: 23.3\n",
      "Epoch: 2, Training loss: 2.23. Test accuracy: 25.66\n",
      "===End of run=== num_epochs: 3, learning_rate: 0.001, acc: 25.66\n",
      "Epoch: 0, Training loss: 2.27. Test accuracy: 19.31\n",
      "===End of run=== num_epochs: 1, learning_rate: 0.01, acc: 19.31\n",
      "Epoch: 0, Training loss: 2.25. Test accuracy: 27.19\n",
      "Epoch: 1, Training loss: 2.12. Test accuracy: 42.96\n",
      "===End of run=== num_epochs: 2, learning_rate: 0.01, acc: 42.96\n",
      "Epoch: 0, Training loss: 2.24. Test accuracy: 18.57\n",
      "Epoch: 1, Training loss: 2.12. Test accuracy: 33.58\n",
      "Epoch: 2, Training loss: 2.04. Test accuracy: 52.17\n",
      "===End of run=== num_epochs: 3, learning_rate: 0.01, acc: 52.17\n",
      "Epoch: 0, Training loss: 1.54. Test accuracy: 65.35\n",
      "===End of run=== num_epochs: 1, learning_rate: 0.1, acc: 65.35\n",
      "Epoch: 0, Training loss: 1.58. Test accuracy: 73.11\n",
      "Epoch: 1, Training loss: 1.12. Test accuracy: 75.82\n",
      "===End of run=== num_epochs: 2, learning_rate: 0.1, acc: 75.82\n",
      "Epoch: 0, Training loss: 1.52. Test accuracy: 68.89\n",
      "Epoch: 1, Training loss: 1.2. Test accuracy: 76.84\n",
      "Epoch: 2, Training loss: 0.97. Test accuracy: 77.7\n",
      "===End of run=== num_epochs: 3, learning_rate: 0.1, acc: 77.7\n"
     ]
    }
   ],
   "source": [
    "with storage.run():\n",
    "    train_loader, test_loader = get_dataloaders(batch_size=100, train_size=1_000)\n",
    "    for learning_rate in [0.001, 0.01, 0.1]:\n",
    "        for num_epochs in [1, 2, 3]:\n",
    "            model, acc = train_lr(train_loader, test_loader, learning_rate, num_epochs)\n",
    "            print(\n",
    "                f\"===End of run=== num_epochs: {num_epochs}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some explaining, but should basically revolve around a \"function\n",
    "calls become (conjunctive) constraints between values\" metaphor. The below is a\n",
    "template for a query that can be extended with more parameters (`batch_size`,\n",
    "`train_size`) over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    train_loader, test_loader = get_dataloaders(batch_size=100, train_size=1_000)\n",
    "    learning_rate = Q().named(\"learning_rate\")\n",
    "    num_epochs = Q().named(\"num_epochs\")\n",
    "    model, acc = train_lr(train_loader, test_loader, learning_rate, num_epochs)\n",
    "    df = q.get_table(learning_rate, num_epochs, acc.named(\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>3</td>\n",
       "      <td>77.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>2</td>\n",
       "      <td>75.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.100</td>\n",
       "      <td>1</td>\n",
       "      <td>65.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>3</td>\n",
       "      <td>52.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010</td>\n",
       "      <td>2</td>\n",
       "      <td>42.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>25.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>19.309999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>13.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>11.059999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>9.969999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  num_epochs   accuracy\n",
       "2          0.100           3  77.699997\n",
       "1          0.100           2  75.820000\n",
       "8          0.100           1  65.349998\n",
       "4          0.010           3  52.169998\n",
       "6          0.010           2  42.959999\n",
       "7          0.001           3  25.660000\n",
       "9          0.010           1  19.309999\n",
       "0          0.001           5  13.620000\n",
       "5          0.001           1  11.059999\n",
       "3          0.001           2   9.969999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"accuracy\"], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e39bb3b1f45b78879464f3858f3ac405da62799496d9b7e0a39caf0b676c9a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
