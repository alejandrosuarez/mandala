{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Logistic regression\n",
    "This tutorial will show you how `mandala` works in a small logistic regression\n",
    "ML project. You'll see how *queriable & composable* memoization is a simple way\n",
    "to achieve the main goals of scientific data management:\n",
    "- easily iterate by just dumping more logic/parameters/experiments on top of the\n",
    "code you already ran. Memoization automatically takes care of loading past\n",
    "results, skipping over past computations, and merging results across compatible\n",
    "versions of your code.\n",
    "- explore the interdependencies of all saved results incrementally and\n",
    "declaratively with **computation frames**, generalized dataframes that operate\n",
    "over memoized computational graphs. Expand the computational history of\n",
    "artifacts backward (to what produced them) and/or forward (to experiments that\n",
    "used them), and generate dataframes of results for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# recommended way to import mandala functionality\n",
    "from mandala._next.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op # memoizing decorator\n",
    "def generate_dataset(random_seed=42):\n",
    "    \"\"\"\n",
    "    Synthetic dataset for binary classification, with 80/20 train/test split\n",
    "    \"\"\"\n",
    "    print(f\"Generating dataset...\")\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=10,\n",
    "        n_informative=8,\n",
    "        random_state=random_seed,\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "@op\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model on the training data\n",
    "    \"\"\"\n",
    "    print(f\"Training model...\")\n",
    "    model = LogisticRegression(max_iter=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, model.score(X_train, y_train)\n",
    "\n",
    "@op\n",
    "def eval_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating model...\")\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the pipeline and adding logic/parameters/experiments on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "To start, just run the pipeline with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amakelov/workspace/current/conda_envs/serimats/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n",
      "Training model...\n",
      "Evaluating model...\n",
      "Train accuracy: AtomRef(0.71625, hid='8f5...', cid='6dc...'), Test accuracy: AtomRef(0.675, hid='57f...', cid='790...')\n"
     ]
    }
   ],
   "source": [
    "storage = Storage() # in-memory storage for all results in this notebook\n",
    "\n",
    "with storage: # we make @ops use a given storage with this `with` block \n",
    "    X_train, X_test, y_train, y_test = generate_dataset()\n",
    "    model, train_acc = train_model(X_train, y_train)\n",
    "    test_acc = eval_model(model, X_test, y_test)\n",
    "    print(f\"Train accuracy: {train_acc}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all three calls are saved to the storage. `@op`s return **value \n",
    "references**, which wrap a Python object with some storage metadata needed to\n",
    "make the memoization *compose*. \n",
    "\n",
    "Thanks to that metadata, when we re-run this code, the storage recognizes\n",
    "step-by-step that all work has already been done, and only loads *references* to\n",
    "the results (not the Python objects themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: AtomRef(hid='8f5...', cid='6dc...', in_memory=False), Test accuracy: AtomRef(hid='57f...', cid='790...', in_memory=False)\n"
     ]
    }
   ],
   "source": [
    "@op\n",
    "def train_model(X_train, y_train, max_iter:int = NewArgDefault(10)):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model on the training data\n",
    "    \"\"\"\n",
    "    print(f\"Training model...\")\n",
    "    model = LogisticRegression(max_iter=max_iter)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, model.score(X_train, y_train)\n",
    "\n",
    "with storage: # we make @ops use a given storage with this `with` block \n",
    "    X_train, X_test, y_train, y_test = generate_dataset()\n",
    "    model, train_acc = train_model(X_train, y_train)\n",
    "    test_acc = eval_model(model, X_test, y_test)\n",
    "    print(f\"Train accuracy: {train_acc}, Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amakelov/workspace/current/conda_envs/serimats/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Evaluating model...\n",
      "Train accuracy: 0.71375, Test accuracy: 0.665\n",
      "Train accuracy: 0.71625, Test accuracy: 0.675\n",
      "Training model...\n",
      "Evaluating model...\n",
      "Train accuracy: 0.71625, Test accuracy: 0.675\n",
      "Training model...\n",
      "Evaluating model...\n",
      "Train accuracy: 0.71625, Test accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "with storage: # same code, but now it only loads pointers to saved results\n",
    "    X_train, X_test, y_train, y_test = generate_dataset()\n",
    "    for max_iter in (1, 10, 100, 1000):\n",
    "        model, train_acc = train_model(X_train, y_train, max_iter=max_iter)\n",
    "        test_acc = eval_model(model, X_test, y_test)\n",
    "        print(f\"Train accuracy: {storage.unwrap(train_acc)}, Test accuracy: {storage.unwrap(test_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComputationFrame with 7 variable(s) (18 unique refs), 3 operation(s) (9 unique calls)\n",
       "Computational graph:\n",
       "    X_train, y_train = generate_dataset(random_seed=random_seed)\n",
       "    output_0, output_1 = train_model(max_iter=max_iter, y_train=y_train, X_train=X_train)\n",
       "    output_0_0 = eval_model(model=output_0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = storage.cf(train_model).expand()\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tuples from the computation graph:\n",
      "X_train, y_train = generate_dataset(random_seed=random_seed)\n",
      "output_0, output_1 = train_model(y_train=y_train, X_train=X_train, max_iter=max_iter)\n",
      "output_0_0 = eval_model(model=output_0)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_seed</th>\n",
       "      <th>generate_dataset</th>\n",
       "      <th>y_train</th>\n",
       "      <th>X_train</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>train_model</th>\n",
       "      <th>output_0</th>\n",
       "      <th>eval_model</th>\n",
       "      <th>output_0_0</th>\n",
       "      <th>output_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>Call(generate_dataset, cid='19a...', hid='c3f....</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...</td>\n",
       "      <td>[[-3.003424127660346, 3.678752891905966, -2.71...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Call(train_model, cid='cf0...', hid='6eb...')</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>Call(eval_model, cid='147...', hid='73d...')</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.71625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>Call(generate_dataset, cid='19a...', hid='c3f....</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...</td>\n",
       "      <td>[[-3.003424127660346, 3.678752891905966, -2.71...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Call(train_model, cid='123...', hid='afe...')</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>Call(eval_model, cid='ec0...', hid='788...')</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.71625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Call(generate_dataset, cid='19a...', hid='c3f....</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...</td>\n",
       "      <td>[[-3.003424127660346, 3.678752891905966, -2.71...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call(train_model, cid='d7d...', hid='ac0...')</td>\n",
       "      <td>LogisticRegression(max_iter=10)</td>\n",
       "      <td>Call(eval_model, cid='7b8...', hid='c5a...')</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.71625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>Call(generate_dataset, cid='19a...', hid='c3f....</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...</td>\n",
       "      <td>[[-3.003424127660346, 3.678752891905966, -2.71...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Call(train_model, cid='163...', hid='9f2...')</td>\n",
       "      <td>LogisticRegression(max_iter=1)</td>\n",
       "      <td>Call(eval_model, cid='39b...', hid='be0...')</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.71375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random_seed                                   generate_dataset  \\\n",
       "0           42  Call(generate_dataset, cid='19a...', hid='c3f....   \n",
       "1           42  Call(generate_dataset, cid='19a...', hid='c3f....   \n",
       "2           42  Call(generate_dataset, cid='19a...', hid='c3f....   \n",
       "3           42  Call(generate_dataset, cid='19a...', hid='c3f....   \n",
       "\n",
       "                                             y_train  \\\n",
       "0  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...   \n",
       "1  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...   \n",
       "2  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...   \n",
       "3  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, ...   \n",
       "\n",
       "                                             X_train  max_iter  \\\n",
       "0  [[-3.003424127660346, 3.678752891905966, -2.71...     100.0   \n",
       "1  [[-3.003424127660346, 3.678752891905966, -2.71...    1000.0   \n",
       "2  [[-3.003424127660346, 3.678752891905966, -2.71...       NaN   \n",
       "3  [[-3.003424127660346, 3.678752891905966, -2.71...       1.0   \n",
       "\n",
       "                                     train_model  \\\n",
       "0  Call(train_model, cid='cf0...', hid='6eb...')   \n",
       "1  Call(train_model, cid='123...', hid='afe...')   \n",
       "2  Call(train_model, cid='d7d...', hid='ac0...')   \n",
       "3  Call(train_model, cid='163...', hid='9f2...')   \n",
       "\n",
       "                            output_0  \\\n",
       "0               LogisticRegression()   \n",
       "1  LogisticRegression(max_iter=1000)   \n",
       "2    LogisticRegression(max_iter=10)   \n",
       "3     LogisticRegression(max_iter=1)   \n",
       "\n",
       "                                     eval_model  output_0_0  output_1  \n",
       "0  Call(eval_model, cid='147...', hid='73d...')       0.675   0.71625  \n",
       "1  Call(eval_model, cid='ec0...', hid='788...')       0.675   0.71625  \n",
       "2  Call(eval_model, cid='7b8...', hid='c5a...')       0.675   0.71625  \n",
       "3  Call(eval_model, cid='39b...', hid='be0...')       0.665   0.71375  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grow the project with new parameters\n",
    "Running the pipeline once is nice, but where `mandala` really shines is in\n",
    "enabling you to grow a computational project in various ways with the minimal\n",
    "necessary code changes, and have the storage interfaces \"just work\". \n",
    "\n",
    "Let's begin exploring this by investigating the effect of changing the learning\n",
    "rate of the model. So far, you have been using the default learning rate of\n",
    "`0.001`. Let's try a few other values, but also see how they compare with the\n",
    "default value. Thanks to memoization, this is easy to do without re-doing\n",
    "expensive work: we can use a list of values for the `learning_rate` parameter\n",
    "that includes the default, and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage:\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for learning_rate in [0.001, 0.01, 0.1]:\n",
    "        model, acc = train_model(train_dataset, test_dataset, learning_rate)\n",
    "        print(\n",
    "            # `unwrap()` is used to get the value wrapped by a `ValueRef`\n",
    "            f\"===end of run=== learning_rate: {learning_rate}, acc: {round(storage.unwrap(acc), 2)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first run was re-used from before, while the 2nd and 3rd were\n",
    "freshly computed. We see that the higher the learning rate, the better the\n",
    "final accuracy. Now, let's try varying the batch size as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage:\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(storage.unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the results\n",
    "By now, you have run the pipeline with many different combinations of\n",
    "parameters, and it's getting difficult to make sense of all the results so far.\n",
    "One option to \"query\" the results is to just re-run the above workflow, or a\n",
    "\"sub-workflow\" of it. \n",
    "\n",
    "For example, how might you get all the results for a given learning rate, e.g.\n",
    "`learning_rate=0.1`? One answer: just by re-running the subset of the above code\n",
    "using this value of the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage:\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.1]:  # only change relative to previous cell\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(storage.unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of storage access pattern is called **retracing**: you \"retrace\"\n",
    "computational code that you have already run in order to recover the quantities\n",
    "computed along the way. You can use retracing to query existing results (like\n",
    "you did above), or to easily add new parameters/logic that need to compute over\n",
    "existing results.\n",
    "\n",
    "However, sometimes you don't have a specific piece of code to retrace and just\n",
    "want to look at all the results in storage. For this, you can use a \"full\n",
    "storage search\" query interface. One option for this is to directly point to\n",
    "variables in the computation. For example, by referring to the `acc` variable\n",
    "from the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.similar(acc, context=True) # this may take a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = storage.cf(train_model).expand()\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened? 🤯\n",
    "Behind the scenes, `mandala` builds a computational graph that links the inputs\n",
    "and outputs to each call (as well as elements of collections to the collection\n",
    "itself, but this is a topic for another tutorial :) ). This means that the last\n",
    "value of `acc` in the loop above \"knows\" that it was computed by a certain\n",
    "composition of memoized functions. \n",
    "\n",
    "This composition serves as the \"shape\" of the query, which looks for\n",
    "computations in the storage that follow this same pattern, but possibly with\n",
    "different values. \n",
    "\n",
    "More formally,\n",
    "- **variables** stand for a value stored in storage. These include the `Q()`\n",
    "placeholder objects in the graph printed out above, as well as the values\n",
    "returned by `@op`-decorated functions in the `storage.query()` block.\n",
    "- **constraints** (i.e. function calls) enforce functional relationships between\n",
    "variables.\n",
    "\n",
    "The result of the query is a table, where each row is a choice of values for \n",
    "the variables that satisfy **all** the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to limit the columns of the table, just omit the `context=True`\n",
    "argument, which will restrict to only the variables you pass in explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.similar(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you have the option of running or editing the query manually by\n",
    "copy-pasting the graph into a `with storage.query()` context. For example, you\n",
    "can give human-readable names to the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a larger dataset, or: how to modify memoized functions gracefully\n",
    "A very common scenario in ML is to extend an existing experimental primitive\n",
    "with new functionality, for example by exposing a hard-coded parameter, or\n",
    "adding a new behavior to the function (e.g., an option to use a different\n",
    "algorithm). With `mandala` you can seamlessly integrate this new functionality\n",
    "with the results of previous runs.\n",
    "\n",
    "For example, suppose you want to improve the accuracy numbers above. So far, you\n",
    "have been running all models on a synthetic dataset of size `1000`. Let's try\n",
    "increasing the dataset size while still sampling from the same distribution and\n",
    "see if that helps! \n",
    "\n",
    "To do this, just directly modify the `generate_dataset` function\n",
    "to take an additional argument `n_samples` with a default value of `1000`. Let's\n",
    "also print out a message when it's called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def generate_dataset(n_samples: int = 1000) -> Tuple[TensorDataset, TensorDataset]:\n",
    "    print(\"Hi from `generate_dataset` with a new argument!\")\n",
    "    x = np.random.randn(n_samples, DATA_DIMENSION)\n",
    "    y = x[:, 0] > 0\n",
    "    x, y = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    train_dataset = TensorDataset(x[:train_size], y[:train_size])\n",
    "    test_dataset = TensorDataset(x[train_size:], y[train_size:])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the memoized calls after we change the function? Find out by\n",
    "re-running the project so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate_dataset` was quiet, indicating that it was not re-run!\n",
    "This is because `mandala` recognized that the value of `n_samples` was the\n",
    "default, so it re-used the call to the previous (zero-argument)\n",
    "`generate_dataset`. This is how adding new inputs to memoized functions works:\n",
    "the default value (which must always be provided for new inputs) is used to\n",
    "distinguish between old and new calls. You can add as many new inputs as you\n",
    "want, as long as you provide a default value for each of them.\n",
    "\n",
    "Let's finally run the pipeline with a larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a nice table with the results; you can do this by minimally\n",
    "modifying the query code from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    n_samples = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, n_samples, acc).sort_values(by='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a very simple way to co-evolve your computational code and the\n",
    "query \"interface\" to its results!\n",
    "\n",
    "However, it looks like the larger dataset didn't help! Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Oh no, there's a bug!\", or: function versioning\n",
    "If you look at the `train_model` function carefully, you'll see there's a\n",
    "subtle but crucial mistake: because the call to `optimizer.step()` happens\n",
    "outside the loop over the batches, the model is only updated once per epoch with\n",
    "the last batch!\n",
    "\n",
    "Unfortunately, we have already ran a lot of experiments with this bug. How can\n",
    "we fix this without messing up our storage and re-running more than we need to?\n",
    "Starting the whole project from scratch is not great, because it means we would\n",
    "have to regenerate the datasets too. \n",
    "\n",
    "Fortunately, `mandala` has a simple way to deal with this situation with the\n",
    "minimal possible amount of code changes. You must do two things:\n",
    "- change the function to fix the bug, and **increment the\n",
    "  version number**. By default, each function starts at version `0`. \n",
    "- re-run the experiments that used the old version of the function. All results\n",
    "  that do not depend on the bug will be reused, and only the results that\n",
    "  depend on values that the bugfix changes will be recomputed.\n",
    "\n",
    "**NOTE**: there is an optional, more fine-grained versioning system that \n",
    "automatically tracks the dependencies of each memoized call for changes, and\n",
    "allows you to effectively do the same as above (and much more), described in the\n",
    "next tutorial.\n",
    "\n",
    "So, fix the bug and increment the version number in the `@op` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(version=1)\n",
    "def train_model(\n",
    "    train_dataset: TensorDataset,\n",
    "    test_dataset: TensorDataset,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 100,\n",
    "    num_epochs: int = 3,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    \"\"\"\n",
    "    Train a logistic model on the given training dataset with the given\n",
    "    hyperparameters.\n",
    "\n",
    "    Prints out the train loss and test accuracy at the end of\n",
    "    each epoch. Returns the trained model and the final test accuracy.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xs, ys in train_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xs)\n",
    "            loss_value = loss(output, ys)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()  #! bugfix here\n",
    "        # test\n",
    "        model.eval()\n",
    "        accurate, total = 0, 0\n",
    "        for xs, ys in test_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            output = model(xs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += ys.size(0)\n",
    "            accurate += (predicted == ys).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, round(float(acc.item()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, literally copy-paste the computation + query code from above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )\n",
    "\n",
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    n_samples = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, n_samples, acc).sort_values(by='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now we have the correct results. Maybe. At least, they make more sense than\n",
    "before: accuracy went up, and the larger dataset actually helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model weights, or: track workflows with complex logic\n",
    "So far, you have been running the same two-part pipeline independently for\n",
    "several sets of parameters. This kind of workflow is common in ML projects, and\n",
    "many libraries provide things like hyperparameter tuning interfaces for such\n",
    "cases (e.g., `sklearn`). \n",
    "\n",
    "However, real-world projects often go beyond these \"embarrassingly DAG-like\"\n",
    "workflows, and require more complex logic, which in turn leads to more complex\n",
    "patterns for computation reuse and data management. \n",
    "\n",
    "This is another place where `mandala` shines: thanks to the transparent way you\n",
    "build experiments directly in plain Python, you are free to make the logic as\n",
    "complex as necessary, and still get a simple way to access the results.\n",
    "\n",
    "To illustrate this, let's compare the trained model weights of models trained\n",
    "with the same batch size and dataset, but different learning rates. First,\n",
    "define a function that computes the distance between the weights of two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def get_model_distance(\n",
    "    model_1: LogisticRegression,\n",
    "    model_2: LogisticRegression,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance between two models' weights\n",
    "    \"\"\"\n",
    "    weight_distance = torch.dist(model_1.linear.weight, model_2.linear.weight)\n",
    "    bias_distance = torch.dist(model_1.linear.bias, model_2.linear.bias)\n",
    "    return round(float(weight_distance + bias_distance), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the mistakes of all the models trained on the synthetic dataset of\n",
    "size `2000` for each batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                model_dist = get_model_distance(model_1, model_2)\n",
    "                print(\n",
    "                    f\"===end of run=== batch_size: {batch_size}, lr_1: {lr_1}, lr_2: {lr_2}, overlap coefficient: {unwrap(model_dist)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, note that thanks to the memoization, the only new computations here\n",
    "were calls to the new `compare_mistakes` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the new workflow\n",
    "How might you get a nice table with the results of the above workflow? Again,\n",
    "you have two options. \n",
    "\n",
    "The simplest is just to retrace the code that you have already run, and collect \n",
    "the results as rows of a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    rows = []\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                model_distance = get_model_distance(model_1, model_2)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"lr_1\": lr_1,\n",
    "                        \"lr_2\": lr_2,\n",
    "                        \"model_distance\": unwrap(model_distance),\n",
    "                    }\n",
    "                )\n",
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(by=[\"model_distance\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `storage.query()` context manager, but you\n",
    "need to be careful to specify the correct constraints! In particular, you must\n",
    "create two placeholders for the learning rate (why?). \n",
    "\n",
    "The easiest way to build the query structure is to copy-paste the computational\n",
    "code, and then modify it to replace iteration over a parameter range with a\n",
    "placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query():\n",
    "    n_samples = Q().named(\"n_samples\")\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    lr_1 = Q().named(\"lr_1\")\n",
    "    lr_2 = Q().named(\"lr_2\")\n",
    "    model_1, acc_1 = train_model(train_dataset, test_dataset, lr_1, batch_size)\n",
    "    model_2, acc_2 = train_model(train_dataset, test_dataset, lr_2, batch_size)\n",
    "    model_distance = get_model_distance(model_1, model_2)\n",
    "df = storage.df(batch_size, lr_1, lr_2, model_distance.named(\"model_distance\"))\n",
    "\n",
    "df.sort_values(by=[\"model_distance\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that more similar learning rates often (but not always) lead to a\n",
    "smaller distance between the corresponding learned models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The project you just finished illustrated all the main features of\n",
    "`mandala`, and how they work together to enable a very simple and direct\n",
    "way of building and querying computational projects. While large-scale ML \n",
    "projects may require some additional patterns for properly using `mandala`, the\n",
    "patterns you have already seen can take you a long way.\n",
    "\n",
    "### The importance of a good function decomposition\n",
    "Finally, it's a good time to reflect on something that often gets overlooked in\n",
    "ML, but is especially important when it comes to effective data management with\n",
    "`mandala`: **function decomposition**. \n",
    "\n",
    "Decomposing a project like the above into functions (i.e., `generate_dataset`,\n",
    "`train_model`, `get_model_distance`) is something you often find yourself doing\n",
    "almost without thinking. It's a necessary practice for managing complexity in\n",
    "software engineering and ML experimentation in general. \n",
    "\n",
    "However, it is also particularly important to find a \"good\" decomposition when\n",
    "tracking experiments with `mandala`, because the kind of decomposition you use\n",
    "has consequences on the performance of your computations and storage system.\n",
    "\n",
    "For example, consider an alternative decomposition with a single function that\n",
    "does both the dataset generation and the model training:\n",
    "```python\n",
    "def generate_and_train(n_samples, batch_size, learning_rate) -> Tuple[LogisticRegression, float]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "A frequent use case is to train many models on the same dataset, but with\n",
    "different hyperparameters. With a single function, you'd have to re-run the\n",
    "dataset generation function every time, even though the dataset is the same!\n",
    "\n",
    "This illustates the main principle of good function decomposition:\n",
    "**functions should be as small as possible, but no smaller**. It's best to pack\n",
    "independent units of work that you may want to combine in different ways into\n",
    "separate functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30c0510467e0bc33a523a84a8acb20ce0730b8eb0ee254a4b0039140f094f217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
